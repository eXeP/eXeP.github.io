<!DOCTYPE html>
<html lang="en">
<head>

  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8">
  <title>Pietari Kaskela</title>
  <meta name="description" content="A post about nvidia-warp and PyTorch.">
  <meta name="keywords" content="NVIDIA, Warp, NVIDIA-Warp, Pytorch, Differentiable">
  <meta name="author" content="Pietari Kaskela">

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link href="//fonts.googleapis.com/css?family=Raleway:400,300,600" rel="stylesheet" type="text/css">

  <!-- CSS
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href="../css/normalize.css">
  <link rel="stylesheet" href="../css/skeleton.css">
  <link rel="stylesheet" href="../css/custom.css">

  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="icon" type="image/png" href="/images/favicon.png">

</head>
<body>

  <!-- Primary Page Layout
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <div class="container" style="margin-bottom: 15%;">
    <nav class="navbar" style="margin-top: 10%;">
      <div>
        <ul class="navbar-list">
          <li class="navbar-item"><a class="navbar-link" href="/">About me</a></li>
          <li class="navbar-item"><a class="navbar-link" href="/blog.html">Blog</a></li>
          <li class="navbar-item"><a class="navbar-link" href="/keyboards.html">Keyboards</a></li>
        </ul>
      </div>
    </nav>
    <div class="row" style="padding: var(--space-m) 0; margin-bottom: var(--space-m); border-bottom: 1px solid #eee;" >
      <h5 style="margin:0"><span style='font-weight: bold;'>CUDA swizzle: avoiding shared memory bank conflicts</span></h5>
      <p style="margin:0"><h7>18.5.2024</h7></p>
      <h7>A simple tutorial on avoiding shared memory bank conflicts using swizzling in CUDA.</h7>
    </div>
    <div class="row">
      <div class="twelve columns">
        <b><p>TL;DR: <a href="https://developer.nvidia.com/blog/efficient-matrix-transpose-cuda-cc/">A legendary technical blog post</a> on how to write an efficient CUDA kernel for matrix transpose contains an easy, but somewhat awkward way to avoid shared memory bank conflicts. There is an (in my opinion) better way, but it's not well documented. I'm here to sing the praises of swizzling.</p></b>

        <p>I was inspired to write this post after endulging in one of my favorite pastimes: procrastinating important stuff by reading blog posts about CUDA. This particular <a href="https://research.colfax-intl.com/tutorial-matrix-transpose-in-cutlass/">blog post</a> was a rewrite of <a href="https://developer.nvidia.com/blog/efficient-matrix-transpose-cuda-cc/">a legendary technical blog post</a> about optimizing CUDA matrix transpose, but using CUTLASS (CUDA C++ template abstractions) instead of pure CUDA C++.</p>

        <p>They use a nice technique called <a href="https://github.com/NVIDIA/cutlass/blob/main/include/cute/swizzle.hpp#L44">swizzling</a>, but glance over the technical details. While we could just ignore the details and use the template library, I think raw-dogging CUDA C++ is much more fun.</p>

        <p>Anyway, what is even the problem we are solving here? Oh, right, shared memory bank conflicts. So, to briefly set the scene here, what if I told you that the super-fast, high-bandwidth and super-expensive HBM global memory powering the top of the line AI chips such as NVIDIA H100, is actually super-fucking-slow and CUDA kernel programming is mostly about avoiding it? This is done by first loading stuff to shared memory (much smaller, but much faster), doing computations and yeeting the results back to the global memory. When we allocate shared memory, it will look like this:</p>
        <div class="row">
          <div class="two columns"><div>Byte index:</div></div>
          <div class="ten columns">
            <div class="grid-row">
                <div class="grid-item">0</div>
                <div class="grid-item">1</div>
                <div class="grid-item">2</div>  
                <div class="grid-item">3</div>
                <div class="grid-item">4</div>
                <div class="grid-item">5</div>  
                <div class="grid-item">6</div>
                <div class="grid-item">7</div>
                <div class="grid-item">8</div>  
                <div class="grid-item">9</div>
                <div class="grid-item">10</div>  
                <div class="grid-item">11</div>
                <div style="padding: var(--space-xxs)">...</div>
                <div class="grid-item">124</div>
                <div class="grid-item">125</div>
                <div class="grid-item">126</div>
                <div class="grid-item">127</div>
                <div class="grid-item">128</div>
                <div class="grid-item">129</div>
                <div class="grid-item">130</div>
                <div class="grid-item">131</div>
                <div style="padding: var(--space-xxs)">...</div>
            </div>
          </div>
        </div>
        <div class="row" style="margin-bottom: var(--space-s);">
          <div class="two columns"><div>Bank:</div></div>
          <div class="ten columns">
            <div class="grid-row">
              <div class="grid-item">0</div>
              <div class="grid-item">0</div>
              <div class="grid-item">0</div>  
              <div class="grid-item">0</div>
              <div class="grid-item">1</div>
              <div class="grid-item">1</div>  
              <div class="grid-item">1</div>
              <div class="grid-item">1</div>
              <div class="grid-item">2</div>  
              <div class="grid-item">2</div>
              <div class="grid-item">2</div>  
              <div class="grid-item">2</div>
              <div style="padding: var(--space-xxs)">...</div>
              <div class="grid-item">31</div>
              <div class="grid-item">31</div>
              <div class="grid-item">31</div>
              <div class="grid-item">31</div>
              <div class="grid-item">0</div>
              <div class="grid-item">0</div>
              <div class="grid-item">0</div>
              <div class="grid-item">0</div>
              <div style="padding: var(--space-xxs)">...</div>
            </div>
          </div>
        </div>
        <p>So the first 4 bytes (32 bits) are assigned to bank 0, the next 4 bytes are assigned to bank 0, etc. There are 32 banks on most NVIDIA GPUs, so after bank 31 the next 4 bytes go again to bank 0. A limitation of the shared memory is that only one thread of a warp (32 threads) can access a bank at a time. Bank conflicts happen when several threads in a warp try to access the same bank during the same cycle and access will become sequential and you code will be super-slow (some exceptions apply, read the CUDA docs for more). Avid readers might wonder why this is the case, in which case I suggest reading <a href="https://developer.nvidia.com/blog/using-shared-memory-cuda-cc/">this post.</a> Anyway, we just have to deal with it.</p>

        <p><a href="https://developer.nvidia.com/blog/efficient-matrix-transpose-cuda-cc/">The legendary blog post</a> leaves us with the following transpose kernel (the full code is available on <a href="https://github.com/NVIDIA-developer-blog/code-samples/blob/master/series/cuda-cpp/transpose/transpose.cu">github</a>):</p>
        <pre><code>__global__ void transposeCoalesced(float *odata, const float *idata)
{
  __shared__ float tile[TILE_DIM][TILE_DIM];

  int x = blockIdx.x * TILE_DIM + threadIdx.x;
  int y = blockIdx.y * TILE_DIM + threadIdx.y;
  int width = gridDim.x * TILE_DIM;

  for (int j = 0; j &lt; TILE_DIM; j += BLOCK_ROWS)
     tile[threadIdx.y+j][threadIdx.x] = idata[(y+j)*width + x];

  __syncthreads();

  x = blockIdx.y * TILE_DIM + threadIdx.x;  // transpose block offset
  y = blockIdx.x * TILE_DIM + threadIdx.y;

  for (int j = 0; j &lt; TILE_DIM; j += BLOCK_ROWS)
     odata[(y+j)*width + x] = tile[threadIdx.x][threadIdx.y + j];
}

int main() {
  const int TILE_DIM = 32;
  const int BLOCK_ROWS = 8;
  const int nx = 1024;
  const int ny = 1024;
  const int mem_size = nx*ny*sizeof(float);
  float *d_idata, *d_cdata, *d_tdata;
  checkCuda( cudaMalloc(&d_idata, mem_size) );
  checkCuda( cudaMalloc(&d_cdata, mem_size) );
  checkCuda( cudaMalloc(&d_tdata, mem_size) );
  dim3 dimGrid(nx/TILE_DIM, ny/TILE_DIM, 1);
  dim3 dimBlock(TILE_DIM, BLOCK_ROWS, 1);
  transposeCoalesced&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(d_tdata, d_idata);
  return 0;
}
        </code></pre>
        <p>The github code contains benchmarking, which handily tells us the performance of the kernel in terms of GB/s on my GTX 1080 Ti to be:<code>coalesced transpose              238.69 GB/s</code>, which is about half of the official maximum bandwidth of 484.4 GB/s. To reach closer to the limit, we will have to fix some shared memory bank conflicts: notice the first shared memory write <code>tile[threadIdx.y+j][threadIdx.x]</code>. The 32 threads in the first warp will span the first 32 block indices, which will be the combinations of <code>threadIdx.x = 0, 1, 2, 3</code> and <code>threadIdx.y = 0, 1, 2, 3, 4, 5, 6, 7</code>.</p>
        <p>(0, 0), j=0 -> tile[threadIdx.y+j][threadIdx.x] -> tile[0 * 32 + 0] -> 0 -> 0 modulo 32</p>
        
      </div>
    </div>
  </div>

<!-- End Document
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
</body>
</html>
