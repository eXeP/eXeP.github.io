<!DOCTYPE html>
<html lang="en">
<head>

  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8">
  <title>Pietari Kaskela</title>
  <meta name="description" content="A post about nvidia-warp and PyTorch.">
  <meta name="keywords" content="NVIDIA, Warp, NVIDIA-Warp, Pytorch, Differentiable">
  <meta name="author" content="Pietari Kaskela">

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link href="//fonts.googleapis.com/css?family=Raleway:400,300,600" rel="stylesheet" type="text/css">

  <!-- CSS
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href="../css/normalize.css">
  <link rel="stylesheet" href="../css/skeleton.css">
  <link rel="stylesheet" href="../css/custom.css">

  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="icon" type="image/png" href="/images/favicon.png">

</head>
<body>

  <!-- Primary Page Layout
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <div class="container" style="margin-bottom: 15%;">
    <nav class="navbar" style="margin-top: 10%;">
      <div>
        <ul class="navbar-list">
          <li class="navbar-item"><a class="navbar-link" href="/">About me</a></li>
          <li class="navbar-item"><a class="navbar-link" href="/blog.html">Blog</a></li>
          <li class="navbar-item"><a class="navbar-link" href="/keyboards.html">Keyboards</a></li>
        </ul>
      </div>
    </nav>
    <div class="row" style="padding: var(--space-m) 0; margin-bottom: var(--space-m); border-bottom: 1px solid #eee;" >
      <h5 style="margin:0"><span style='font-weight: bold;'>CUDA swizzle: avoiding shared memory bank conflicts</span></h5>
      <p style="margin:0"><h7>18.5.2024</h7></p>
      <h7>A simple tutorial on avoiding shared memory bank conflicts using swizzling in CUDA.</h7>
    </div>
    <div class="row">
      <div class="twelve columns">
        <b><p>TL;DR: <a href="https://developer.nvidia.com/blog/efficient-matrix-transpose-cuda-cc/">A legendary technical blog post</a> on how to write an efficient CUDA kernel for matrix transpose contains an easy, but somewhat awkward way to avoid shared memory bank conflicts. There is an (in my opinion) better way, but it's not well documented. I'm here to sing the praises of swizzling.</p></b>

        <p>I was inspired to write this post after endulging in one of my favorite pastimes: procrastinating important stuff by reading blog posts about CUDA. This particular <a href="https://research.colfax-intl.com/tutorial-matrix-transpose-in-cutlass/">blog post</a> was a rewrite of <a href="https://developer.nvidia.com/blog/efficient-matrix-transpose-cuda-cc/">a legendary technical blog post</a> about optimizing CUDA matrix transpose, but using CUTLASS (CUDA C++ template abstractions) instead of pure CUDA C++.</p>

        <p>They use a nice technique called <a href="https://github.com/NVIDIA/cutlass/blob/main/include/cute/swizzle.hpp#L44">swizzling</a>, but glance over the technical details. While we could just ignore the details and use the template library, I think raw-dogging CUDA C++ is much more fun.</p>

        <p>Anyway, what is even the problem we are solving here? Oh, right, shared memory bank conflicts. So, to briefly set the scene here, what if I told you that the super-fast, high-bandwidth and super-expensive HBM global memory powering the top of the line AI chips such as NVIDIA H100, is actually super-fucking-slow and CUDA kernel programming is mostly about avoiding it? This is done by first loading stuff to shared memory (much smaller, but much faster), doing the computations and yeeting the results back to the global memory.</p>

        <p>Now, when we allocate shared memory, it will look like this:</p>
        <div class="row">
          <div class="two columns"><div>Byte index:</div></div>
          <div class="ten columns">
            <div class="grid-row">
                <div class="grid-item">0</div>
                <div class="grid-item">1</div>
                <div class="grid-item">2</div>  
                <div class="grid-item">3</div>
                <div class="grid-item">4</div>
                <div class="grid-item">5</div>  
                <div class="grid-item">6</div>
                <div class="grid-item">7</div>
                <div class="grid-item">8</div>  
                <div class="grid-item">9</div>
                <div class="grid-item">10</div>  
                <div class="grid-item">11</div>
                <div style="padding: var(--space-xxs)">...</div>
                <div class="grid-item">124</div>
                <div class="grid-item">125</div>
                <div class="grid-item">126</div>
                <div class="grid-item">127</div>
                <div class="grid-item">128</div>
                <div class="grid-item">129</div>
                <div class="grid-item">130</div>
                <div class="grid-item">131</div>
                <div style="padding: var(--space-xxs)">...</div>
            </div>
          </div>
        </div>
        <div class="row" style="margin-bottom: var(--space-s);">
          <div class="two columns"><div>Bank:</div></div>
          <div class="ten columns">
            <div class="grid-row">
              <div class="grid-item">0</div>
              <div class="grid-item">0</div>
              <div class="grid-item">0</div>  
              <div class="grid-item">0</div>
              <div class="grid-item">1</div>
              <div class="grid-item">1</div>  
              <div class="grid-item">1</div>
              <div class="grid-item">1</div>
              <div class="grid-item">2</div>  
              <div class="grid-item">2</div>
              <div class="grid-item">2</div>  
              <div class="grid-item">2</div>
              <div style="padding: var(--space-xxs)">...</div>
              <div class="grid-item">31</div>
              <div class="grid-item">31</div>
              <div class="grid-item">31</div>
              <div class="grid-item">31</div>
              <div class="grid-item">0</div>
              <div class="grid-item">0</div>
              <div class="grid-item">0</div>
              <div class="grid-item">0</div>
              <div style="padding: var(--space-xxs)">...</div>
            </div>
          </div>
        </div>
        <p>So the first 4 bytes (32 bits) are assigned to bank 0, the next 4 bytes are assigned to bank 0, etc. There are 32 banks on most NVIDIA GPUs, so after bank 31 the next 4 bytes go again to bank 0. Avid readers might wonder why this is the case, in which case I suggest reading <a href="https://developer.nvidia.com/blog/using-shared-memory-cuda-cc/">this post.</a> Anyway, as a wise man once said, it is what it is.</p>

        <p>A limitation of the shared memory is that only one thread of a warp (32 threads) can access a bank at a time. Bank conflicts happen when several threads in a warp try to access the same bank during the same cycle and the limitation causes the access to become sequential. As a result, your code will be super-slow (some exceptions apply, read the CUDA docs for more).</p>

        <p><a href="https://developer.nvidia.com/blog/efficient-matrix-transpose-cuda-cc/">The legendary blog post</a> leaves us with the following bank-conflictey transpose kernel (I highly recommend checking out the <a href="https://github.com/NVIDIA-developer-blog/code-samples/blob/master/series/cuda-cpp/transpose/transpose.cu">code</a> to understand how this kernel is called):</p>
        <pre><code>__global__ void transposeCoalesced(float *odata, const float *idata)
{
  __shared__ float tile[TILE_DIM][TILE_DIM];
    
  int x = blockIdx.x * TILE_DIM + threadIdx.x;
  int y = blockIdx.y * TILE_DIM + threadIdx.y;
  int width = gridDim.x * TILE_DIM;

  for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS)
     tile[threadIdx.y+j][threadIdx.x] = idata[(y+j)*width + x];

  __syncthreads();

  x = blockIdx.y * TILE_DIM + threadIdx.x;  // transpose block offset
  y = blockIdx.x * TILE_DIM + threadIdx.y;

  for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS)
     <b>odata[(y+j)*width + x] = tile[threadIdx.x][threadIdx.y + j];</b>
}
        </code></pre>
        <p>This code causes bank conflicts, because the <code>TILE_DIM = 32</code> coincides with the number of shared memory banks, leading to all threads of a warp accessing the same bank at the same time (happens on the bolded line of code). Let's see how this actually happens in the first warp during the first iteration of the loop over TILE_DIM:</p>
        <table class="u-full-width">
          <thead>
            <tr>
              <th>threadIdx.x</th>
              <th>threadIdx.y</th>
              <th>tile[threadIdx.x][threadIdx.y + j]</th>
              <th>bank</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>0</td>
              <td>0</td>
              <td>tile[0][0]</td>
              <td>0</td>
            </tr>
            <tr>
              <td>1</td>
              <td>0</td>
              <td>tile[1][0]</td>
              <td>0</td>
            </tr>
            <tr>
              <td>2</td>
              <td>0</td>
              <td>tile[2][0]</td>
              <td>0</td>
            </tr>
            <tr>
              <td>⋮</td>
              <td>⋮</td>
              <td>⋮</td>
              <td>⋮</td>
            </tr>
            <tr>
              <td>31</td>
              <td>0</td>
              <td>tile[31][0]</td>
              <td>0</td>
            </tr>
          </tbody>
        </table>
        <p>As you can see, all of the reads are to the same bank, which will cause sequential access and a slowdown. A simple trick to make the code non-bank-conflictey is by uncoinciding the shared memory banks and TILE_DIM:</p>
        <pre><code>__global__ void transposeNoBankConflicts(float *odata, const float *idata)
{
  __shared__ float tile[TILE_DIM][TILE_DIM<b>+1</b>];
  ...</code></pre>
        <p>However, this will use some extra memory, and makes me feel dirty. Benchmarking this code with a matrix of size 2048 by 2048 and averaging over a thousand runs yields us the following result: </p>
        <pre><code>coalesced transpose              357.58
conflict-free transpose          401.39</code></pre>
        <p>Which is about 74% and 83%, respectively, of my GTX 1080 Ti's official maximum bandwidth 484.4 GB/s. So simply by adding that <code>+1</code> to the shared memory tile dimension, our bandwidth increased by quite a lot. But what if I don't want to allocate a bunch of unused memory? Well, then we <b>swizzle</b>:</p>
        <pre><code>__global__ void transposeNoBankConflictsSwizzle(float *odata, const float *idata)
{
  <b>__shared__ float tile[TILE_DIM][TILE_DIM];</b>
    
  int x = blockIdx.x * TILE_DIM + threadIdx.x;
  int y = blockIdx.y * TILE_DIM + threadIdx.y;
  int width = gridDim.x * TILE_DIM;

  for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS)
      <b>tile[threadIdx.y+j][threadIdx.x^(threadIdx.y+j)] = idata[(y+j)*width + x];</b>

  __syncthreads();

  x = blockIdx.y * TILE_DIM + threadIdx.x;  // transpose block offset
  y = blockIdx.x * TILE_DIM + threadIdx.y;

  for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS)
      <b>odata[(y+j)*width + x] = tile[threadIdx.x][(threadIdx.y + j)^threadIdx.x];</b>
}</code></pre>
        <p>But what's actually going on here? The xor-operator provides convenient one-to-one mapping inside each row of the shared memory tile. Here's how it changes the table above:</p>
        <table class="u-full-width">
          <thead>
            <tr>
              <th>threadIdx.x</th>
              <th>threadIdx.y</th>
              <th>tile[threadIdx.x][(threadIdx.y + j)^threadIdx.x]</th>
              <th>bank</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>0</td>
              <td>0</td>
              <td>tile[0][0]</td>
              <td>0</td>
            </tr>
            <tr>
              <td>1</td>
              <td>0</td>
              <td>tile[1][1]</td>
              <td>1</td>
            </tr>
            <tr>
              <td>2</td>
              <td>0</td>
              <td>tile[2][2]</td>
              <td>2</td>
            </tr>
            <tr>
              <td>⋮</td>
              <td>⋮</td>
              <td>⋮</td>
              <td>⋮</td>
            </tr>
            <tr>
              <td>31</td>
              <td>0</td>
              <td>tile[31][31]</td>
              <td>31</td>
            </tr>
          </tbody>
        </table>
        <p>As you can see, now there are zero bank conflicts inside of the warp!</p>
        <p>Benchmarking using the same parameters yields us a similar bandwidth, but without the unused shared memory allocation: 
        <pre><code>conflict-free transpose swizzle              402.46</code></pre>
        <p>Which is in the same ballpark as the extra memory trick.</p>
      </div>
    </div>
  </div>

<!-- End Document
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
</body>
</html>
