<!DOCTYPE html>
<html lang="en">
<head>

  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8">
  <title>Pietari Kaskela</title>
  <meta name="description" content="A post about nvidia-warp and PyTorch.">
  <meta name="keywords" content="NVIDIA, Warp, NVIDIA-Warp, Pytorch, Differentiable">
  <meta name="author" content="Pietari Kaskela">

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link href="//fonts.googleapis.com/css?family=Raleway:400,300,600" rel="stylesheet" type="text/css">

  <!-- CSS
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href="../css/normalize.css">
  <link rel="stylesheet" href="../css/skeleton.css">
  <link rel="stylesheet" href="../css/custom.css">

  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="icon" type="image/png" href="/images/favicon.png">

</head>
<body>

  <!-- Primary Page Layout
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <div class="container" style="margin-bottom: 15%;">
    <nav class="navbar" style="margin-top: 25%;">
      <div>
        <ul class="navbar-list">
          <li class="navbar-item"><a class="navbar-link" href="/">About me</a></li>
          <li class="navbar-item"><a class="navbar-link" href="/blog.html">Blog</a></li>
          <li class="navbar-item"><a class="navbar-link" href="/keyboards.html">Keyboards</a></li>
        </ul>
      </div>
    </nav>
    <div class="row" style="padding: 2rem 0; margin-bottom:2rem; border-bottom: 1px solid #eee;" >
      <h5 style="margin:0"><span style='font-weight: bold;'>Filling in missing data with NVIDIA-Warp</span></h5>
      <p style="margin:0"><h7>15.2.2024</h7></p>
      <h7>Basically me fangirling over NVIDIA-Warp. You should try it too. Includes Warp-PyTorch interop example.</h7>
    </div>
    <div class="row">
      <div class="twelve columns">
        <b><p>TL;DR: <a href="https://github.com/NVIDIA/warp">NVIDIA-Warp</a> (<code>pip install warp-lang</code>) is a high-performance framework for writing differentiable parallel code. Think of cuda, but much less tedious to write! Checkout out the <a href="https://github.com/NVIDIA/warp/tree/main/examples">examples</a> and <a href="https://nvidia.github.io/warp/">documentation</a> to learn more.</p></b>
        <p>While working on a <a href="https://proceedings.bmvc2023.org/501/">problem with sparse data</a> recently, I had a case where some of the data was not initialized. In order to pass it to a neural network, I decided I wanted to initialize the missing data to the spatially closest valid data:</p>
        <div class="row2" style="padding-bottom: 2rem;">
          <div class="column2">
            <div class="grid-container">
              <div class="grid-item">2</div>
              <div class="grid-item">NaN</div>
              <div class="grid-item">NaN</div>  
              <div class="grid-item">4</div>
              <div class="grid-item">NaN</div>
              <div class="grid-item">3</div>  
              <div class="grid-item">NaN</div>
              <div class="grid-item">7</div>
              <div class="grid-item">NaN</div>  
              <div class="grid-item">NaN</div>
              <div class="grid-item">1</div>  
              <div class="grid-item">NaN</div>
              <div class="grid-item">NaN</div>  
              <div class="grid-item">NaN</div>
              <div class="grid-item">1</div>  
              <div class="grid-item">NaN</div>
            </div>
          </div>
          <div class="column2">
            <div class="arrow-box">
              &rarr;
            </div>
          </div>
          <div class="column2">
            <div class="grid-container">
              <div class="grid-item">2</div>
              <div class="grid-item">3</div>
              <div class="grid-item">4</div>  
              <div class="grid-item">4</div>
              <div class="grid-item">2</div>
              <div class="grid-item">3</div>  
              <div class="grid-item">1</div>
              <div class="grid-item">7</div>
              <div class="grid-item">3</div>  
              <div class="grid-item">3</div>
              <div class="grid-item">1</div>  
              <div class="grid-item">7</div>
              <div class="grid-item">1</div>  
              <div class="grid-item">1</div>
              <div class="grid-item">1</div>  
              <div class="grid-item">1</div>
            </div>
          </div>
        </div>
        <p>I decided that this infilling-process should also be fast and differentiable, but I was not concerned about ties. Below I have a visualization of the ties (same number) when we consider infilling the center pixel of a 5-by-5 grid.</p> 
        <div class="grid-container5" style="padding-bottom: 2rem;">
          <div class="grid-item" style="background-color:#555">5</div>  
          <div class="grid-item" style="background-color:#666">4</div>
          <div class="grid-item" style="background-color:#888">3</div>  
          <div class="grid-item" style="background-color:#666">4</div>
          <div class="grid-item" style="background-color:#555">5</div>  

          <div class="grid-item" style="background-color:#666">4</div>  
          <div class="grid-item" style="background-color:#aaa">2</div>
          <div class="grid-item" style="background-color:#ccc">1</div>  
          <div class="grid-item" style="background-color:#aaa">2</div>
          <div class="grid-item" style="background-color:#666">4</div>  

          <div class="grid-item" style="background-color:#888">3</div>  
          <div class="grid-item" style="background-color:#ccc">1</div>
          <div class="grid-item" style="background-color:#eee">NaN</div>  
          <div class="grid-item" style="background-color:#ccc">1</div>
          <div class="grid-item" style="background-color:#888">3</div>  

          <div class="grid-item" style="background-color:#666">4</div>  
          <div class="grid-item" style="background-color:#aaa">2</div>
          <div class="grid-item" style="background-color:#ccc">1</div>  
          <div class="grid-item" style="background-color:#aaa">2</div>
          <div class="grid-item" style="background-color:#666">4</div>  

          <div class="grid-item" style="background-color:#555">5</div>  
          <div class="grid-item" style="background-color:#666">4</div>
          <div class="grid-item" style="background-color:#888">3</div>  
          <div class="grid-item" style="background-color:#666">4</div>
          <div class="grid-item" style="background-color:#555">5</div>  
        </div>
        <p>However, I really couldn't figure out how to do this in PyTorch, short of writing the forward and backward passes in cuda. Lucky for me, the problem is rather easy with Warp:</p>
        <pre><code>import warp as wp
import numpy as np

wp.init()

@wp.func
def check_bounds(i: wp.int32, j: wp.int32, n: wp.int32, m: wp.int32):
    return i >= 0 and j >= 0 and i < n and j < m

@wp.kernel
def _fill_in_kernel(
    feat: wp.array2d(dtype=wp.float32),
    mask: wp.array2d(dtype=wp.uint8),
    output: wp.array2d(dtype=wp.float32)
):
    i, j = wp.tid()
    # The feature is already valid, no need to infill
    if mask[i, j] == 0:
        output[i, j] = feat[i, j]
        return
    
    max_len = wp.max(feat.shape[0], feat.shape[1])
    n = feat.shape[0]
    m = feat.shape[1]
    for l in range(1, max_len+1):
        if check_bounds(i+l, j, n, m) and mask[i+l, j] == 0:
            output[i, j] = feat[i+l, j]
            return
        if check_bounds(i-l, j, n, m) and mask[i-l, j] == 0:
            output[i, j] = feat[i-l, j]
            return
        if check_bounds(i, j+l, n, m) and mask[i, j+l] == 0:
            output[i, j] = feat[i, j+l]
            return
        if check_bounds(i, j-l, n, m) and mask[i, j-l] == 0:
            output[i, j] = feat[i, j-l]
            return
        for o in range (1, l+1):
            if check_bounds(i+l, j+o, n, m) and mask[i+l, j+o] == 0:
                output[i, j] = feat[i+l, j+o]
                return
            if check_bounds(i+l, j-o, n, m) and mask[i+l, j-o] == 0:
                output[i, j] = feat[i+l, j-o]
                return
            if check_bounds(i-l, j+o, n, m) and mask[i-l, j+o] == 0:
                output[i, j] = feat[i-l, j+o]
                return
            if check_bounds(i-l, j-o, n, m) and mask[i-l, j-o] == 0:
                output[i, j] = feat[i-l, j-o]
                return
            if check_bounds(i+o, j+l, n, m) and mask[i+o, j+l] == 0:
                output[i, j] = feat[i+o, j+l]
                return
            if check_bounds(i-o, j+l, n, m) and mask[i-o, j+l] == 0:
                output[i, j] = feat[i-o, j+l]
                return
            if check_bounds(i+o, j-l, n, m) and mask[i+o, j-l] == 0:
                output[i, j] = feat[i+o, j-l]
                return
            if check_bounds(i-o, j-l, n, m) and mask[i-o, j-l] == 0:
                output[i, j] = feat[i-o, j-l]
                return
    
f = np.array([[2, 0, 0, 4], [0, 3, 0, 7], [0, 0, 1, 0], [0, 0, 1, 0]])
# zeros as missing values marked here
# we could just do without this mask, but I wanted to support a mask so here we are
m = (f == 0).astype(np.uint8)
dim = f.shape

# allocate arrays for warp
feat = wp.array2d(f, dtype=wp.float32)
mask = wp.array2d(m, dtype=wp.uint8)
output = wp.zeros(dim, dtype=wp.float32)

print("Running kernel with dim {}".format(dim))
# launch kernel
wp.launch(
    kernel=_fill_in_kernel,
    dim=dim,
    inputs=[feat, mask],
    outputs=[output]
)
print(feat)
print(output)
        </code></pre>
        <pre><code>Output:
[[2. 0. 0. 4.]
[0. 3. 0. 7.]
[0. 0. 1. 0.]
[0. 0. 1. 0.]]
[[2. 3. 4. 4.]
[2. 3. 1. 7.]
[3. 3. 1. 7.]
1. 1. 1. 1.]]</code></pre>
        <p>Actually, in hindsight, this was probably not the best example. But hey, at least it was easier than cuda! Also, in my case the problem itself existed only because I had not yet figured out bilinear splatting.</p>
        <p>Now, to make the warp-kernel differentiable and interoperable with PyTorch, we need to wrap it in a <code>torch.autograd.Function</code>.</p>
        <pre><code>import torch
class NearestFill(torch.autograd.Function):
    @staticmethod
    def forward(ctx, f, mask):
        height, width = f.shape
        device = wp.device_from_torch(f.device)

        wp_output = wp.zeros(shape=f.shape)
        tape = wp.Tape() # this object keeps track of gradients in warp
        ctx.tape = tape
        ctx.wp_output = wp_output
        ctx.wp_f = wp.from_torch(f)
        ctx.wp_mask = wp.from_torch(mask)

        wp.synchronize_device(device)
        with tape:
            wp.launch(
                kernel=_fill_in_kernel,
                dim=(height, width),
                inputs=[ctx.wp_f, ctx.wp_mask],
                outputs=[wp_output],
                device=device
            )
        wp.synchronize_device(device)
        
        return wp.to_torch(wp_output)

    @staticmethod
    def backward(ctx, adj_output):
        ctx.tape.backward(grads={ctx.wp_output: wp.from_torch(adj_output.contiguous())})
        grad_f = None
        if ctx.wp_f in ctx.tape.gradients:
            grad_f = wp.to_torch(ctx.tape.gradients[ctx.wp_f])
        # return should match the parameters to forward
        # we don't need gradient for mask here
        return (grad_f, None)

f_torch = torch.tensor(f, dtype=torch.float32, requires_grad=True).cuda()
m_torch = torch.tensor(m).cuda()
filled_torch = NearestFill.apply(f_torch, m_torch)
print(f_torch)
print(m_torch)
print(filled_torch)</code></pre>
<pre><code>Output:
tensor([[2., 0., 0., 4.],
  [0., 3., 0., 7.],
  [0., 0., 1., 0.],
  [0., 0., 1., 0.]], device='cuda:0', grad_fn=<ToCopyBackward0>)
tensor([[0, 1, 1, 0],
  [1, 0, 1, 0],
  [1, 1, 0, 1],
  [1, 1, 0, 1]], device='cuda:0', dtype=torch.uint8)
tensor([[2., 3., 4., 4.],
  [2., 3., 1., 7.],
  [3., 3., 1., 7.],
  [1., 1., 1., 1.]], device='cuda:0', grad_fn=<NearestFillBackward>)</code></pre>
          <p>Much easier than writing cuda and compiling the PyTorch extensions! Now, I do admit that this particular problem wasn't the best example, but where this more traditional parallel programming really shines is interpolation or anything related to graphics. For example, bicubic interpolation is much easier to implement in Warp than in pure PyTorch.</p>
      </div>
    </div>
  </div>

<!-- End Document
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
</body>
</html>
